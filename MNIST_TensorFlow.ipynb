{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Hello World deep neural network: MNIST Classification\n",
    "\n",
    "## We will detect which digit is written in the MNIST dataset (handwritten digit recognition).\n",
    "- We will use 70,000 images (28x28 pixels) of handwritten digits (1 digit per image) from tensorflow_datasets\n",
    "- We will classify 'each' image into one of 10 categories  (0, 1, 2, 3, 4, 5, 6, 7, 8, 9).\n",
    "- We will use a neural network with 2 hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#There is a PyPI package containing data set (images, videos, ....) files\n",
    "#for tensorflow, the package name is tensorflow_datasets\n",
    "import tensorflow_datasets as tf_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'test': <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>,\n",
       "  'train': <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>},\n",
       " tfds.core.DatasetInfo(\n",
       "     name='mnist',\n",
       "     version=3.0.0,\n",
       "     description='The MNIST database of handwritten digits.',\n",
       "     homepage='http://yann.lecun.com/exdb/mnist/',\n",
       "     features=FeaturesDict({\n",
       "         'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
       "         'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "     }),\n",
       "     total_num_examples=70000,\n",
       "     splits={\n",
       "         'test': 10000,\n",
       "         'train': 60000,\n",
       "     },\n",
       "     supervised_keys=('image', 'label'),\n",
       "     citation=\"\"\"@article{lecun2010mnist,\n",
       "       title={MNIST handwritten digit database},\n",
       "       author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "       journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n",
       "       volume={2},\n",
       "       year={2010}\n",
       "     }\"\"\",\n",
       "     redistribution_info=,\n",
       " ))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = tf_ds.load(name='mnist', as_supervised=True, with_info=True )\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>,\n",
       " 'train': <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    version=3.0.0,\n",
       "    description='The MNIST database of handwritten digits.',\n",
       "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "    }),\n",
       "    total_num_examples=70000,\n",
       "    splits={\n",
       "        'test': 10000,\n",
       "        'train': 60000,\n",
       "    },\n",
       "    supervised_keys=('image', 'label'),\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "- The raw_data is a two elements tuple. \n",
    "- The first component contains the data itself and the second coponent contains info.\n",
    "- We are going to extract the data and the info separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset, mnist_info = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    version=3.0.0,\n",
       "    description='The MNIST database of handwritten digits.',\n",
       "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "    }),\n",
       "    total_num_examples=70000,\n",
       "    splits={\n",
       "        'test': 10000,\n",
       "        'train': 60000,\n",
       "    },\n",
       "    supervised_keys=('image', 'label'),\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>,\n",
       " 'train': <DatasetV1Adapter shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "- The mnist_dataset data set is a dictionary with 2 keys 'train' and 'test'\n",
    "- we have to create our own validation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "# we set the number of validation sample\n",
    "# we use 10% validation and 90% train\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples \n",
    "\n",
    "# we have to convert it to integer\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6000, shape=(), dtype=int64) tf.Tensor(10000, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(num_validation_samples,\n",
    "      num_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalling the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling an image matrix\n",
    "def scale(image, label):\n",
    "    '''scale the pixel and returns: pixel (float32 matrix), label'''\n",
    "    image = tf.cast(image, dtype=tf.float32)\n",
    "    image /= 255.\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "#scaling all images in a dataset using map\n",
    "scaled_train_validation_data = mnist_train.map(scale)\n",
    "scaled_test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schuffling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a global constant BUFFER_SIZE\n",
    "#the data will usually be too big to be schuffled at once\n",
    "# so we shuffle it chunk by chunk and BUFFER_SIZE is the size of a chunk\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "schuffled_train_validation = scaled_train_validation_data.shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the train an validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = schuffled_train_validation.take(num_validation_samples)\n",
    "train_data = schuffled_train_validation.skip(num_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifying the batch size\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples) \n",
    "test_data = scaled_test_data.batch(num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6000,), dtype=int64, numpy=array([1, 8, 0, ..., 5, 0, 6])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Training part\n",
    "<center><b>Data ---> Model ---> Objective function ---> Optimization algorithm</b></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>The Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have to stack multiple layers on top of each other in a Sequential instance object \n",
    "\n",
    "### Hyperparameters\n",
    "#size of a single layer\n",
    "layer_size = 150 #50\n",
    "input_size = 784  # since we have 28 * 28 =784 pixels for one image\n",
    "output_size = 10 #since we want to recognize 10 digits: 0 to 9\n",
    "\n",
    "# first we have to flatten each 28x28 matrix into a vector of length 784\n",
    "# we use 'relu', rectified linear unit,  as activation function for the two hidden layers\n",
    "# we use 'softmax' activation function for the output layer since we are doing classification\n",
    "model = tf.keras.Sequential([\n",
    "                             tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "                             tf.keras.layers.Dense(layer_size, activation='relu'),\n",
    "                             tf.keras.layers.Dense(layer_size, activation='relu'),\n",
    "                             tf.keras.layers.Dense(output_size, activation='softmax'),\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>The Objective function and the Optimization algorithm</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_optimizer = tf.keras.optimizers.Adam()\n",
    "my_loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "#sparse categorical crossentropy applies one-hot encoding \n",
    "#i.e convert the 10 categories into 10 standard basis of R^10\n",
    "\n",
    "# compiling the model\n",
    "model.compile(optimizer=my_optimizer, loss=my_loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Training </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Fitting the model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "540/540 - 9s - loss: 0.2957 - accuracy: 0.9156 - val_loss: 0.1488 - val_accuracy: 0.9573\n",
      "Epoch 2/15\n",
      "540/540 - 9s - loss: 0.1166 - accuracy: 0.9649 - val_loss: 0.0994 - val_accuracy: 0.9710\n",
      "Epoch 3/15\n",
      "540/540 - 9s - loss: 0.0797 - accuracy: 0.9756 - val_loss: 0.0788 - val_accuracy: 0.9767\n",
      "Epoch 4/15\n",
      "540/540 - 9s - loss: 0.0578 - accuracy: 0.9821 - val_loss: 0.0732 - val_accuracy: 0.9798\n",
      "Epoch 5/15\n",
      "540/540 - 9s - loss: 0.0470 - accuracy: 0.9854 - val_loss: 0.0534 - val_accuracy: 0.9845\n",
      "Epoch 6/15\n",
      "540/540 - 9s - loss: 0.0369 - accuracy: 0.9883 - val_loss: 0.0521 - val_accuracy: 0.9850\n",
      "Epoch 7/15\n",
      "540/540 - 9s - loss: 0.0312 - accuracy: 0.9902 - val_loss: 0.0438 - val_accuracy: 0.9868\n",
      "Epoch 8/15\n",
      "540/540 - 8s - loss: 0.0262 - accuracy: 0.9921 - val_loss: 0.0356 - val_accuracy: 0.9885\n",
      "Epoch 9/15\n",
      "540/540 - 9s - loss: 0.0199 - accuracy: 0.9939 - val_loss: 0.0338 - val_accuracy: 0.9900\n",
      "Epoch 10/15\n",
      "540/540 - 9s - loss: 0.0173 - accuracy: 0.9945 - val_loss: 0.0263 - val_accuracy: 0.9913\n",
      "Epoch 11/15\n",
      "540/540 - 10s - loss: 0.0160 - accuracy: 0.9948 - val_loss: 0.0217 - val_accuracy: 0.9935\n",
      "Epoch 12/15\n",
      "540/540 - 8s - loss: 0.0164 - accuracy: 0.9945 - val_loss: 0.0267 - val_accuracy: 0.9920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe48fbe7210>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 15\n",
    "VALIDATION_STEPS = num_validation_samples / BATCH_SIZE\n",
    "VALIDATION_STEPS = tf.cast(VALIDATION_STEPS, tf.int64)\n",
    "\n",
    "my_callback = tf.keras.callbacks.EarlyStopping()\n",
    "\n",
    "# argument callbacks must be a list, if not we get the erro\n",
    "#TypeError: 'EarlyStopping' object is not iterable\n",
    "# train_data contains both inputs and targets\n",
    "model.fit(train_data,\n",
    "          epochs=NUM_EPOCHS,\n",
    "          validation_data=(validation_inputs, validation_targets),\n",
    "          validation_steps=VALIDATION_STEPS,\n",
    "          callbacks= [my_callback],\n",
    "          verbose=2)\n",
    "#ValueError: `batch_size` or `steps` is required for `Tensor` or `NumPy` input data.\n",
    "#to avoid this error we need to specify the \"validation steps\": validation_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Testing / Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the model by using the method <b>.evaluate()</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0901 - accuracy: 0.9779\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09012626856565475"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9779000282287598"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Predicting in Deployment</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_result = model.predict_on_batch(test_data)\n",
    "#test_result_np = test_result.numpy()\n",
    "#test_result_np.round(1)\n",
    "test_result = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.15202026e-16, 3.31078602e-11, 1.00000000e+00, ...,\n",
       "        9.43522604e-14, 2.22625304e-11, 5.89938099e-18],\n",
       "       [1.00000000e+00, 1.50231786e-15, 9.18348569e-16, ...,\n",
       "        1.52118281e-13, 2.35420299e-13, 1.73108530e-14],\n",
       "       [3.34317022e-15, 8.11343614e-12, 4.27930295e-14, ...,\n",
       "        7.49771889e-09, 2.49219048e-13, 1.13883993e-06],\n",
       "       ...,\n",
       "       [3.12734834e-08, 6.01175643e-09, 4.82653959e-05, ...,\n",
       "        3.56560996e-11, 9.99951720e-01, 2.29180591e-10],\n",
       "       [9.99994397e-01, 1.82687102e-10, 5.65142591e-06, ...,\n",
       "        7.68762998e-10, 3.06850492e-13, 8.87734886e-10],\n",
       "       [1.02824169e-13, 9.14863888e-13, 6.59161690e-13, ...,\n",
       "        7.25800099e-17, 1.50805507e-10, 1.09250855e-11]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_result = test_result.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-tensorflow2.0",
   "language": "python",
   "name": "python3-tensorflow2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
